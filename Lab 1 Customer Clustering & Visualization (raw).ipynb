{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>"],"metadata":{"id":"aB0XJW3Jhmzd"},"id":"aB0XJW3Jhmzd"},{"cell_type":"markdown","id":"4fd2312e-f344-44f2-ac8a-fbf30b80673b","metadata":{"id":"4fd2312e-f344-44f2-ac8a-fbf30b80673b","tags":[]},"source":["# Lab 1: Customer Clustering & Visualization"]},{"cell_type":"markdown","id":"d777dda9-111b-466f-9001-1da0f12f0ce1","metadata":{"id":"d777dda9-111b-466f-9001-1da0f12f0ce1","tags":[]},"source":["## Objective & Description:\n","This Lab focuses on clustering and visualizing bank customers' data, which includes variables such as date of birth (DOB), account balance, transactions, transaction amount, and transaction place. We will use publicly available data from a bank in India to learn how to preprocess data and to prepare it for clustering and visualization.\n","\n","The objective of the lab is to equip participants with the skills to analyze and gain insights from customer data, which they can use to improve business strategies and decision-making processes. Participants will learn how to use Python programming language libraries such as Pandas, NumPy, and Scikit-learn to preprocess the data, perform clustering using algorithms such as K-Means, Hierarchical Clustering, and DBSCAN, and visualize the results using techniques such as scatter plots, 3D plots, and heatmaps.\n","\n","If you don't know Python don't worry we've got you backed up here are some resources to learn python:\n","- [Introduction to Python - DataCamp](https://www.datacamp.com/courses/intro-to-python-for-data-science)\n","- [Python for Data Science: Fundamentals  - DataQuest](https://www.dataquest.io/course/python-for-data-science-fundamentals/)\n","- [Python for Data Science: Intermediate  - DataQuest](https://www.dataquest.io/course/python-for-data-science-intermediate/)\n","- [Maybe you prefer books](https://pythonbooks.org/free-books/)\n","\n","\n","## Tasks:\n","The workshop consists of multiple tasks which have to be solved sequentially:\n","\n","1. **Data cleaning.** Clean the bank customer data by handling missing values, identifying and handling outliers, and transforming the data as needed. \n","\n","2. **Data exploration.** Use appropriate visualization techniques to identify patterns in the data. Create visualizations that clearly and accurately represent the relationships between different features of the data. This is critical to assess your trained model in the next step.)\n","\n","3. **Train your clustering model.** Apply clustering techniques to the preprocessed data and compare the results of at least two different clustering algorithms. Calculate accuracy of your model predictions and the overall quality of clustering models. \n","\n","4. **Visualize clustering results.** Create visualizations that accurately represent the clustering results. Use appropriate visualization techniques to highlight the differences between different clusters and provide insights into the patterns present in the data.\n","\n","5. **Data interpretation and conclusions.** Interpret and analyze the results of their clustering models and visualizations. Draw meaningful insights from analysis and present findings in a clear and concise manner.\n","\n","Follow the steps on this Jupyter notebook.\n"]},{"cell_type":"markdown","id":"746e10ee-6d95-4fc0-a987-1f073dba108d","metadata":{"id":"746e10ee-6d95-4fc0-a987-1f073dba108d"},"source":["### Data Cleaning"]},{"cell_type":"markdown","id":"64c8a193-e660-43a6-a087-92a22e7db908","metadata":{"id":"64c8a193-e660-43a6-a087-92a22e7db908","tags":[]},"source":["#### Task 1.1\n","We have preloaded the dataset into this environment. You are expected to do the following:\n","\n","1. Load the dataset to the notebook as a pandas DataFrame.\n","<details>\n","    <summary>hint:</summary>\n","Pandas has the method `read_csv`.\n","Look for the csv file in the side panel copy its path and load it using the previous method.\n","</details>\n","\n","2. Inspect the data.\n","<details>\n","    <summary>hint:</summary>\n","Pandas Dataframe has the method `head`\n","</details>\n","3. You are provided with the function `data_status`. Call it on the DataFrame. What do you inspect ?\n","<details>\n","    <summary>hint:</summary>\n","Maybe wrong types?\n","</details>\n","4. Look at the info of your data. What do you realize about the numbers ? \n","<details>\n","    <summary>hint:</summary>\n","Pandas DataFrame has the method `info`. invoke it on your data. hint: Any null values? \n","</details>"]},{"cell_type":"code","execution_count":null,"id":"51850ce7-924c-4424-a538-48c8e64d5801","metadata":{"id":"51850ce7-924c-4424-a538-48c8e64d5801"},"outputs":[],"source":["!git clone https://github.com/khengari77/Lab-1-Customer-Clustering-Visualization.git\n","!pip install umap-learn"]},{"cell_type":"code","execution_count":null,"id":"49658315-3931-474d-934f-717c6f316e40","metadata":{"id":"49658315-3931-474d-934f-717c6f316e40","tags":[]},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"764d004a-8038-480a-bf7f-94fdfeca3117","metadata":{"id":"764d004a-8038-480a-bf7f-94fdfeca3117"},"outputs":[],"source":["# load your data by passing the csv file path to the read_csv function.\n","transactions = pd.read_csv('')"]},{"cell_type":"code","execution_count":null,"id":"11ae2f1f-61d2-421c-9498-a94286f650d8","metadata":{"id":"11ae2f1f-61d2-421c-9498-a94286f650d8"},"outputs":[],"source":["def data_status(df):\n","    columns = df.columns\n","    stat = []\n","    for column in columns:\n","        data_type = type(df[column][0])\n","        num_dups = np.sum(df[column].duplicated())\n","        num_null = np.sum(df[column].isna())\n","        stat.append([column, data_type, num_dups, num_null])\n","    status_frame = pd.DataFrame(stat)\n","    status_frame.columns = ['column', 'data type', 'no. duplicates', 'no. null']\n","    return status_frame"]},{"cell_type":"code","execution_count":null,"id":"4ea1bae7-3535-43ac-b62b-76e409d90eda","metadata":{"id":"4ea1bae7-3535-43ac-b62b-76e409d90eda"},"outputs":[],"source":["# Solve Task 1.1.1 here\n"]},{"cell_type":"code","execution_count":null,"id":"2db30c45-21da-4175-969b-1ed5592a2402","metadata":{"id":"2db30c45-21da-4175-969b-1ed5592a2402"},"outputs":[],"source":["# Solve Task 1.1.2 here"]},{"cell_type":"code","execution_count":null,"id":"40779511-ece2-43c7-9abb-b6368d1becf3","metadata":{"id":"40779511-ece2-43c7-9abb-b6368d1becf3"},"outputs":[],"source":["# Solve Task 1.1.2 here"]},{"cell_type":"markdown","id":"0c13a8cb-25b9-41ce-a339-d9494147c723","metadata":{"id":"0c13a8cb-25b9-41ce-a339-d9494147c723","tags":[]},"source":["#### Task 1.2\n","What did you notice? Probably some missing values, wrong types and feature we don't need?\n","Drop any null values and columns that we don't need.\n","<details>\n","    <summary>hint:</summary>\n","    For drop null values we have the method `dropna`. For dropping a certain column or row we have the drop method. Remember you can always look in the pandas documentation."]},{"cell_type":"code","execution_count":null,"id":"ad53482f-8bd2-4746-bb45-d7b4f2279b4f","metadata":{"id":"ad53482f-8bd2-4746-bb45-d7b4f2279b4f","tags":[]},"outputs":[],"source":["# Solve Task 1.2 here"]},{"cell_type":"markdown","id":"6fc185a0-be8f-49ab-b389-5af1a78008c2","metadata":{"id":"6fc185a0-be8f-49ab-b389-5af1a78008c2"},"source":["#### Task 1.3\n","It is always said that data cleaning makes up 80% of a data scientist work. We have quite some cleaning to do if we want to get the most out of our data.\n","\n","Convert columns with wrong types to a more suitable form. The DOB column was processed for you. You only have to work through the other columns. Feel free to add any other columns that you induce from the data or drop any column that you are finished with (extracted the useful data). \n","\n","**Remember it's always a good practice to make a copy from your DataFrame after each major manipulation** use the method `copy`. \n","\n","**Also, remember to inspect your data after each step using the methods in (Task 1.1).** They are quite handy. They also show you if your process succeeded or not.\n","<details>\n","    <summary>hint:</summary>\n","    1. Columns in Pandas DataFrames are basically Pandas Series. A great feature of Pandas Series is that you can apply functions to them in an element-wise fashion. Suppose we have a data series that has a scale form 0 - 2 and we want to scale it from 0 - 1 you can do something like: `series/2` \n","     <br> <br>\n","    2. Pandas has a function : to_datetime that converts any date or time representation in another form to DateTime object\n","     <br> <br>\n","    3. Rare categories should be dealt with either by putting them in a one \"UNKNOWN\" category or by using any statistical method to eliminate them.\n","</details>"]},{"cell_type":"code","execution_count":null,"id":"d5d7ec99-122a-4d06-bd0d-41c2f5599fa0","metadata":{"id":"d5d7ec99-122a-4d06-bd0d-41c2f5599fa0"},"outputs":[],"source":["# feel free to change the variable names.\n","transactions = transactions[transactions['CustomerDOB']\\\n","              .str \\\n","              .fullmatch(\"\\d{1,2}[\\/]\\d{1,2}[\\/]\\d{2}\")]\n","# We used regex pattern matching here because there was some dates \n","# that had an incorrect string representation. \n","# filtered the dataframe and removed any samples such.\n","# Filtering in pandas is quite intuitive it has the following general syntax\n","# df = df[predicate_func(df)]"]},{"cell_type":"code","execution_count":null,"id":"4889c938-3948-446f-8ded-edaf46f6c513","metadata":{"id":"4889c938-3948-446f-8ded-edaf46f6c513"},"outputs":[],"source":["# We could have used pd.to_datetime \n","# but we have dates that are before 1/1/1969\n","# which aren't supported by unix so we use string methods\n","# See Pandas docs \n","# https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n","dates = cleaned_transactions['CustomerDOB'].copy().str.split(\"/\")"]},{"cell_type":"code","execution_count":null,"id":"f882eaaf-3cb7-40c8-ba7c-abd5448f792b","metadata":{"id":"f882eaaf-3cb7-40c8-ba7c-abd5448f792b","scrolled":true,"tags":[]},"outputs":[],"source":["# This data was collected in 2016 so we can assume \n","# that any date birth between 00 - 09 is in the 21st century. \n","# This might not be true for all cases but it won't be harmful \n","# because the percentage of such cases is negligible.\n","\n","def full_year(i):\n","    if i > 9:  return 1900 + i\n","    else: return 2000 + i\n","\n","year = dates.apply(lambda x: full_year(int(x[2])))"]},{"cell_type":"code","execution_count":null,"id":"67a65f72-f3a1-4bc6-a1ae-e66ba25f1c5b","metadata":{"id":"67a65f72-f3a1-4bc6-a1ae-e66ba25f1c5b"},"outputs":[],"source":["# Solve task 1.3 here & below\n","# you can add a new cell by pressing ESC then b on your keyboard"]},{"cell_type":"markdown","id":"ea17c3a3-0ad5-4162-9e70-2ecd2ee63628","metadata":{"id":"ea17c3a3-0ad5-4162-9e70-2ecd2ee63628"},"source":["#### Data exploration:\n","Really, there is no limit to how you can explore data other than your imagination. You can get some metrics of your data by invoking the `describe`. A better way to explore data is by visualizations. [Matplotlib](https://matplotlib.org/stable/index.html) & [Seaborn](https://seaborn.pydata.org/) are two famous library when it comes to data visualizations. Here are some ideas:\n","- correlation matrix.\n","- age distribution.\n","- gender distribution.\n","- income distribution.\n","- transactions amount vs income.\n","- number of transactions vs income.\n","- income vs age.\n","- income vs gender\n","- time vs transactions amount\n","- date vs transactions amount\n","- time vs number of transactions\n","- date vs number of transactions\n","\n","Feel free to do other investigations. Don't limit your imagination!"]},{"cell_type":"code","execution_count":null,"id":"465ad55a-695b-4668-b3b4-5c41256476a4","metadata":{"id":"465ad55a-695b-4668-b3b4-5c41256476a4","tags":[]},"outputs":[],"source":["# Now we have a clean dataset that we can work with. \n","# Let's start by importing some visualization libraries.\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib\n","font = {'weight' : 'bold',\n","        'size'   : 16}\n","\n","matplotlib.rc('font', **font)"]},{"cell_type":"code","execution_count":null,"id":"8d46aa5c-e8ee-43d4-ba8b-416b31eb3eda","metadata":{"id":"8d46aa5c-e8ee-43d4-ba8b-416b31eb3eda"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"97e18cac-2fa4-44ce-8798-0f3990c78564","metadata":{"id":"97e18cac-2fa4-44ce-8798-0f3990c78564"},"source":["### Data Preprocessing\n","Most artificial intelligence models take numbers as their inputs, but we have categorical data in our data. We also have time data. To solve this problem we have to convert these categories to numbers. A way to do that is by [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). This method basically makes each category a column itself and assigns 1 to the sample its present in and 0 if it isn't. In a case where we have binary categorical data (like our gender data) we can basically omit one of the categories.\n","\n","\n","The best order of operations for speeding runtime is \n"," 1. Copying the location column.\n"," 2. Applying One-Hot encoding to the gender and locations.\n","    Note that you don't need to use the One-Hot object for gender you can apply the operation in place try using the `apply` method\n"," 3. Dropping the string columns.\n"," 4. Normalizing the columns.\n"," 5. Concatenating the location data in its one-hot format.\n"," 6. Reducing the dimension of the data.\n","I don't say that this order is the correct but it worked best for me."]},{"cell_type":"code","execution_count":null,"id":"d86ad4bd-5451-4086-83c0-ba884ad759b8","metadata":{"id":"d86ad4bd-5451-4086-83c0-ba884ad759b8"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, minmax_scale\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","id":"91f66c38-01d3-42fb-935f-6c378977091b","metadata":{"id":"91f66c38-01d3-42fb-935f-6c378977091b"},"source":["#### Task 3.1\n","\n","Convert the location data and the gender data to one-hot encoding format"]},{"cell_type":"code","execution_count":null,"id":"4d03a388-718a-4181-972b-daccc187d7c6","metadata":{"id":"4d03a388-718a-4181-972b-daccc187d7c6"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"930904bb-471a-4bd6-8d95-c38410813f54","metadata":{"id":"930904bb-471a-4bd6-8d95-c38410813f54"},"source":["#### Task 3.2\n","\n","Convert the time and date data into a numerical representation\n","<details>\n","    <summary>hint:</summary>\n","Consider values that repeat and effect the data. Data that doesn't change or has a minimal change can be ignored. For example all our data is from 2016 so we can ignore the year.\n","</details>\n"]},{"cell_type":"code","execution_count":null,"id":"5f38a2bb-6d27-4b80-8f4d-1116b165c36e","metadata":{"id":"5f38a2bb-6d27-4b80-8f4d-1116b165c36e"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"c6cacd04-c4f8-4f62-bed0-183886ea9342","metadata":{"id":"c6cacd04-c4f8-4f62-bed0-183886ea9342"},"source":["#### Task 3.3\n","It is usually better to normalize our data in a controlled range for example (0,1). Scikit-learn toolkit has various tools for normalizing data. One of the famous methods is min-max scaling. Apply it on your data per feature. "]},{"cell_type":"code","execution_count":null,"id":"9f7e3ed1-15af-4f34-9e10-04171b1a3d6f","metadata":{"id":"9f7e3ed1-15af-4f34-9e10-04171b1a3d6f"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"77bc658a-0bca-4672-aea5-cfe5bc1da4ba","metadata":{"id":"77bc658a-0bca-4672-aea5-cfe5bc1da4ba"},"source":["#### Task 3.4\n","Inspect your data now. WOW! those are a lot of features. Data with high dimensionality is hard to train and results in poor results. Fortunately there are techniques to reduce the dimensionality of data such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) and [autoencoders](https://en.wikipedia.org/wiki/Autoencoder). Now using PCA reduce the number of features in your data to a sensible number. \n","\n","Note: *Reducing the dimensions of your data isn't a lossless operation. You will lose a lot of data. It's a matter of losing unneeded data. The truth is that the number of features is a hyperparameter and likely you will have to experiment with different numbers until you find a suitable one.*"]},{"cell_type":"code","execution_count":null,"id":"65fd3228-7ffa-48f6-9bae-05b5ce06338b","metadata":{"id":"65fd3228-7ffa-48f6-9bae-05b5ce06338b"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"64427a17-6fff-4a9c-bee5-23776c325394","metadata":{"id":"64427a17-6fff-4a9c-bee5-23776c325394","tags":[]},"source":["### Training your model\n","At last you have reached to the main! That was a long journey wasn't it?! There are many clustering algorithms that are in use in the ML industry: Kmeans, DBSCAN, and Hierarchical Clustering are a few to name. You are required to implement K-means on the data. Choosing the right number of clusters can be tricky but fortunately there are heuristics that help us determine a good number of clusters. Two of the most popular methods are: \n","1. The elbow method\n","2. The Silhouette Method\n","If you want to learn more read [here](https://towardsdatascience.com/how-many-clusters-6b3f220f0ef5)\n","\n","You are provided with a function that calculates the silhouette score for a range of Ks starting from 2 until the number you specify. \n","\n","I will leave the actual interpretation of the silhouette score as an exercise for the reader to search for."]},{"cell_type":"code","execution_count":null,"id":"0270667d-af53-45f3-ac47-441b013bade7","metadata":{"id":"0270667d-af53-45f3-ac47-441b013bade7","tags":[]},"outputs":[],"source":["from sklearn.cluster import KMeans \n","from sklearn.metrics import silhouette_score\n","from sklearn.utils import resample\n","import matplotlib.cm as cm"]},{"cell_type":"code","execution_count":null,"id":"153a169e-665c-4c65-b778-de51c49b8f9e","metadata":{"id":"153a169e-665c-4c65-b778-de51c49b8f9e","tags":[]},"outputs":[],"source":["def silhouette_plot(data, max_clusters=16, n_samples=256, rounds=50):\n","    silhouette_scores = []\n","    for k in range(2,max_clusters):\n","        clusterer = KMeans(n_clusters=k, n_init='auto')\n","        cluster_labels = clusterer.fit_predict(data)\n","        mean_score = 0\n","        for i in range(rounds):\n","             mean_score += silhouette_score(data, cluster_labels, sample_size=n_samples)\n","        mean_score /= rounds\n","        silhouette_scores.append(mean_score)\n","        print(f\"For n_clusters = {k} The average silhouette_score is:\\\n","              {silhouette_scores[k - 2]}\")\n","    plt.figure(figsize=(20, 8))\n","    plt.plot(range(2, max_clusters), silhouette_scores)\n","    plt.title(\"Silhouette Score per K\")\n","    plt.xlabel(\"K\")\n","    plt.ylabel(\"Silhouette Score\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"536fc3ff-c745-4069-9f91-d4591e4c7ed7","metadata":{"id":"536fc3ff-c745-4069-9f91-d4591e4c7ed7"},"outputs":[],"source":["# Run the silhouette_plot here"]},{"cell_type":"code","execution_count":null,"id":"aa35ac07-7180-47b0-9928-51f989c8ffd1","metadata":{"id":"aa35ac07-7180-47b0-9928-51f989c8ffd1"},"outputs":[],"source":["# Implement a K-means clusterer here. \n","# There are few hyperparameters that you can tweak.\n","# Look at scikit-learn documentation."]},{"cell_type":"markdown","id":"7702df1e-73cc-4507-bc53-8d40a33c9227","metadata":{"id":"7702df1e-73cc-4507-bc53-8d40a33c9227"},"source":["### Evaluate the results of clustering\n","In most cases you really don't have well defined categories so you can't evaluate your model like you would do in supervised learning. Nonetheless we still have [other methods](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) that we can use to reason about our model. One of those methods is [Calinski-Harabasz Index](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index). Reading the documentation in the previous link use this method on your model. What do you think about the results?"]},{"cell_type":"code","execution_count":null,"id":"5972e206-94dc-4e9e-b1e1-cd04832efad7","metadata":{"id":"5972e206-94dc-4e9e-b1e1-cd04832efad7"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"c01fc8e2-2a7a-482e-ba6c-89e5f3a05e23","metadata":{"id":"c01fc8e2-2a7a-482e-ba6c-89e5f3a05e23"},"source":["Another useful way to reason about your model is by visualizing the clusters. Unfortunately our model is in higher dimensions so we can't visualize the result with normal methods. A common technique used in these situations is the [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). TSNE in its essence is a dimensionality reduction algorithm but it is used quite often to convert variables to 2D space for visualization. Unfortunately this method takes hours to terminate sometimes. So we will another method namely [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) to reduce our data to 2D space and plot your results using libraries like matplotlib."]},{"cell_type":"code","execution_count":null,"id":"be86bf00-7a1a-4a9f-8bd7-162fc8e3f040","metadata":{"id":"be86bf00-7a1a-4a9f-8bd7-162fc8e3f040"},"outputs":[],"source":["from umap import UMAP"]},{"cell_type":"code","execution_count":null,"id":"08e31210-f31e-4dee-8995-b7824c846f3c","metadata":{"id":"08e31210-f31e-4dee-8995-b7824c846f3c"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"4FQOLd8O57sk","metadata":{"id":"4FQOLd8O57sk"},"source":["## Finished early?\n","Don't worry we've got you backed up. Here are few things to consider:\n","- We talked about reducing dimensionality methods. We used PCA but it has its drawbacks the most serious one is that it assumes that the data is linear which is usually not the case. A more powerful method for reducing dimensionality is using autoencoders. Autoencoders are a variant of neural networks that learn how to represent the data in a smaller dimension using machine learning methods. [Read here about autoencoders and try implementing one by yourself](https://blog.keras.io/building-autoencoders-in-keras.html)\n","\n","- In this notebook we used K-means. K-means has two serious drawbacks:\n","      1. It assumes spherical clusters.\n","      2. It is sensitive to noise and outliers.\n","      \n","\n","If you want to read further: [Click here](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages).\n","Our data had some outliers that made K-means not the preferred method for this problem. A more resilient method to outliers is DBSCAN. [Read here about DBSCAN and try applying it by yourself](https://scikit-learn.org/stable/modules/clustering.html)"]},{"cell_type":"markdown","id":"b721c45b-1893-4c20-b455-d80ee06f490d","metadata":{"id":"b721c45b-1893-4c20-b455-d80ee06f490d"},"source":["## Give us your feedback!\n","[Please fill in this form and give us your feedback](https://forms.gle/hg9VRLUdnocU4G3Y7)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}